{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Airport Data Quality Check and Cleaning (`run_full_qc.ipynb`)\n",
    "\n",
    "This notebook cleans the **Airport Codes** dataset as part of the Capital One Airline Route Profitability Challenge.\n",
    "\n",
    "We aim to:\n",
    "- Validate and standardize raw input fields\n",
    "- Detect and clean invalid data (textual numbers, special characters, inconsistent types)\n",
    "- Impute missing values\n",
    "- Remove null-heavy columns\n",
    "- Handle outliers\n",
    "- Save the cleaned data for further analysis and joins\n",
    "\n",
    "---\n",
    "\n",
    "### Steps Overview:\n",
    "1. Load raw airport data\n",
    "2. Analyze type conversion issues\n",
    "3. Clean numeric columns\n",
    "4. Recheck formatting issues\n",
    "5. Check and visualize null patterns\n",
    "6. Classify columns by type\n",
    "7. Filter to U.S. medium/large airports\n",
    "8. Impute missing values\n",
    "9. Drop null-heavy or empty columns\n",
    "10. Handle duplicates\n",
    "11. Final null check and imputation\n",
    "12. Detect and cap outliers\n",
    "13. Save cleaned data\n",
    "\n",
    "### Step 1: Load Raw Data\n",
    "\n",
    "Load raw airport data using `load_airport_data()` utility.  \n",
    "Also generate an automatic data profiling report using `ydata-profiling`.\n",
    "\n",
    "### Step 2: Initial Type Conversion Checks\n",
    "\n",
    "Use `analyze_conversion_errors()` to detect:\n",
    "- Textual numbers (e.g., \"Thirty Five\")\n",
    "- Malformed floats (e.g., \"8.9abc\")\n",
    "- Invalid date or object formats\n",
    "\n",
    "Print a column-wise error report using `print_error_report()`.\n",
    "\n",
    "\n",
    "### Step 3: Clean Numerical Columns\n",
    "\n",
    "Use `clean_numeric_string()` and apply it to all columns that should be numeric.  \n",
    "Cleans:\n",
    "- Text values like \"Thirty One\"\n",
    "- Floats with extra characters\n",
    "\n",
    "\n",
    "### Step 4: Re-check Type Conversions\n",
    "\n",
    "Run `analyze_conversion_errors()` again to confirm issues were fixed.  \n",
    "If everything looks good, continue to null analysis.\n",
    "\n",
    "\n",
    "### Step 5: Visualize and Analyze Null Patterns\n",
    "\n",
    "Use `plot_missing_values()` and `check_nulls()` to:\n",
    "- Visualize missingness (matrix, bar, dendrogram)\n",
    "- Identify columns with missing > 50%\n",
    "\n",
    "### Step 6: Classify Columns by Data Type\n",
    "\n",
    "Use `classify_columns()` to break down:\n",
    "- Categorical columns\n",
    "- Numerical columns\n",
    "- Ordinal (if any)\n",
    "\n",
    "This helps define proper imputation logic.\n",
    "\n",
    "\n",
    "### Step 7: Filter Dataset to Relevant Airports\n",
    "\n",
    "Filter only:\n",
    "- Airports of type: `medium_airport`, `large_airport`\n",
    "- Country: `US`\n",
    "\n",
    "Use `filter_dataset_based_on_user()` to isolate the relevant airport rows.\n",
    "\n",
    "### Step 8: Impute Missing Values\n",
    "\n",
    "Use `multi_strategy_imputer()` to fill:\n",
    "- Numerical columns: with median\n",
    "- Categorical columns: with \"Unknown\"\n",
    "\n",
    "\n",
    "### Step 9: Drop Null-heavy or Fully-null Columns\n",
    "\n",
    "Drop:\n",
    "- Columns where **100% of values** are null using `drop_all_null_columns()`\n",
    "- Columns with >99% missing using `drop_high_null_columns(threshold=99)`\n",
    "\n",
    "\n",
    "### Step 10: Handle Duplicates\n",
    "\n",
    "Check for:\n",
    "- Fully duplicated rows\n",
    "- Duplicate rows based on key columns (if needed)\n",
    "\n",
    "Use `check_and_handle_duplicates()` with or without subset keys.\n",
    "\n",
    "\n",
    "### Step 11: Final Null Imputation\n",
    "\n",
    "Use `check_nulls()` and reclassify any remaining columns with NA.  \n",
    "Re-impute using appropriate strategies.\n",
    "\n",
    "\n",
    "### Step 12: Detect and Cap Outliers (IQR Method)\n",
    "\n",
    "Use `outlier_summary_report()` and `batch_plot_outliers()` to:\n",
    "- Summarize skewness, kurtosis, and number of outliers\n",
    "- Plot box/KDE plots\n",
    "\n",
    "Cap using `cap_and_report_outliers()` (MAD-based).\n",
    "\n",
    "\n",
    "### Save Final Cleaned Dataset\n",
    "\n",
    "Save cleaned airport dataset to:\n",
    "`/data/cleaned/airports_cleaned.csv`\n",
    "\n",
    "This file will be used for joining with Flights and Tickets data in downstream notebooks.\n",
    "\n",
    "\n",
    "### Generalized Pipeline for All Three Datasets\n",
    "\n",
    "The data quality and cleaning process was applied **consistently across all three datasets** using the **same modular codebase** located in `src/`. This ensured standardization, reusability, and reproducibility across the challenge.\n",
    "\n",
    "#### Reusable Functions from `data_cleaner.py` and `plot_utils.py`:\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `analyze_conversion_errors()` | Detects formatting and type conversion issues in numeric, string, and date fields |\n",
    "| `clean_numeric_string()` | Handles messy numeric values like \"Thirty Five\" or \"8.9abc\" |\n",
    "| `apply_cleaning_to_numeric_columns()` | Applies cleaning logic across selected columns |\n",
    "| `check_nulls()` | Generates null count and percentage report |\n",
    "| `multi_strategy_imputer()` | Imputes missing values using mean/median (numeric) or mode/constant (categorical) |\n",
    "| `drop_all_null_columns()` / `drop_high_null_columns()` | Drops columns with high or full null values |\n",
    "| `check_and_handle_duplicates()` | Detects and optionally removes duplicate rows |\n",
    "| `classify_columns()` | Classifies columns as categorical or numerical for imputation and EDA |\n",
    "| `filter_dataset_based_on_user()` | Applies domain filters like US-only, medium/large airports |\n",
    "| `predictive_categorical_imputer()` | Optional imputation using ML model (e.g. `RandomForestClassifier`) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Go up one level from notebooks/ to project root, then into src/\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(\" src/ path added:\", src_path)\n",
    "\n",
    "# Now import WITHOUT the `src.` prefix\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport, compare\n",
    "from data_cleaner import (\n",
    "    analyze_conversion_errors, print_error_report,\n",
    "    apply_cleaning_to_numeric_columns, check_nulls,\n",
    "    classify_columns, filter_dataset_based_on_user,\n",
    "    multi_strategy_imputer, drop_all_null_columns,\n",
    "    drop_high_null_columns, check_and_handle_duplicates\n",
    ")\n",
    "from plot_utils import (\n",
    "    batch_plot_outliers, outlier_summary_report,\n",
    "    cap_outliers, print_outlier_summary,cap_and_report_outliers,plot_missing_values\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airports QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# === 1. Load Raw Data ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_dir = os.path.join(project_root, 'data', 'raw')\n",
    "file_path = os.path.join(data_dir, 'Airport_Codes.csv')\n",
    "print(f\"\\n Loading data from: {file_path}\")\n",
    "ac = pd.read_csv(file_path)\n",
    "\n",
    "# === 2. Initial Conversion Error Analysis ===\n",
    "convert_dtypes = {\n",
    "    'TYPE': 'string', 'NAME': 'string', 'ELEVATION_FT': 'float',\n",
    "    'CONTINENT': 'string', 'ISO_COUNTRY': 'string',\n",
    "    'MUNICIPALITY': 'string', 'IATA_CODE': 'string', 'COORDINATES': 'string'\n",
    "}\n",
    "report = ProfileReport(df=ac, title=\"Airports Data\")\n",
    "report.to_notebook_iframe()\n",
    "\n",
    "df_cleaned, error_reports, _ = analyze_conversion_errors(ac, convert_dtypes)\n",
    "print_error_report(error_reports)\n",
    "\n",
    "# === 3. Apply Cleaning ===\n",
    "numeric_columns = ['ELEVATION_FT']\n",
    "ac = apply_cleaning_to_numeric_columns(ac, numeric_columns)\n",
    "\n",
    "print(\"Error check after conversion\")\n",
    "# === 4. Recheck Conversion ===\n",
    "df_cleaned, error_reports, _ = analyze_conversion_errors(ac, convert_dtypes)\n",
    "print_error_report(error_reports)\n",
    "\n",
    "# === 5. Null Checks ===\n",
    "plot_missing_values(ac)\n",
    "null_values, na_cols = check_nulls(ac)\n",
    "\n",
    "# === 6. Column Classification ===\n",
    "categorical_dict_cols = classify_columns(ac)\n",
    "\n",
    "# === 7. Filtering ===\n",
    "filter_dataset_dict = {'TYPE': ['medium_airport', 'large_airport'], 'ISO_COUNTRY': ['US']}\n",
    "ac, cat_na_cols, num_na_cols = filter_dataset_based_on_user(ac, filter_dataset_dict, na_cols)\n",
    "\n",
    "# === 8. Imputation ===\n",
    "ac = multi_strategy_imputer(ac, num_cols=num_na_cols, cat_cols=cat_na_cols,\n",
    "                                num_strategy='median', cat_strategy='constant', constant_fill='Unknown')\n",
    "\n",
    "# === 9. Drop NULL Columns ===\n",
    "plot_missing_values(ac)\n",
    "ac, _ = drop_all_null_columns(ac)\n",
    "ac, _ = drop_high_null_columns(ac, threshold=99)\n",
    "\n",
    "# === 10. Deduplication ===\n",
    "ac, _ = check_and_handle_duplicates(ac)\n",
    "\n",
    "# === 11. Final NA Imputation ===\n",
    "null_values, na_cols = check_nulls(ac)\n",
    "cat_na_dict = classify_columns(ac[na_cols])\n",
    "ac = multi_strategy_imputer(ac,\n",
    "                            num_cols=cat_na_dict['numerical'],\n",
    "                            cat_cols=cat_na_dict['categorical'],\n",
    "                            num_strategy='mean', cat_strategy='constant', constant_fill='Unknown')\n",
    "\n",
    "# === 12. Outlier Handling ===\n",
    "outlier_cols = categorical_dict_cols['numerical']\n",
    "print(\"\\n Generating Outlier Summary Report...\")\n",
    "outlier_report_df = outlier_summary_report(ac, outlier_cols, method='iqr')\n",
    "print(outlier_report_df)\n",
    "batch_plot_outliers(ac, outlier_cols, method='iqr')\n",
    "\n",
    "for col in outlier_cols:\n",
    "    ac = cap_and_report_outliers(ac, column=col, k=1)\n",
    "    #print_outlier_summary(ac, col, k=3)\n",
    "batch_plot_outliers(ac, outlier_cols, method='iqr')\n",
    "\n",
    "# === 13. Save Final Output ===\n",
    "cleaned_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'cleaned', 'airports_cleaned.csv'))\n",
    "ac.to_csv(cleaned_path, index=False)\n",
    "print(f\"\\n Cleaned dataset saved to: {cleaned_path}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === Load Flights Data ===\n",
    "    file_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'raw', 'Flights.csv'))\n",
    "    print(f\"\\n Loading data from: {file_path}\")\n",
    "    ac = pd.read_csv(file_path)\n",
    "\n",
    "    # === Dtype Mapping ===\n",
    "    convert_dtypes = {\n",
    "        'FL_DATE': 'date', 'OP_CARRIER': 'string', 'TAIL_NUM': 'string', 'OP_CARRIER_FL_NUM': 'string',\n",
    "        'ORIGIN_AIRPORT_ID': 'string', 'ORIGIN': 'string', 'ORIGIN_CITY_NAME': 'string',\n",
    "        'DEST_AIRPORT_ID': 'string', 'DESTINATION': 'string', 'DEST_CITY_NAME': 'string',\n",
    "        'DEP_DELAY': 'float', 'ARR_DELAY': 'float', 'CANCELLED': 'float',\n",
    "        'AIR_TIME': 'float', 'DISTANCE': 'float', 'OCCUPANCY_RATE': 'float'\n",
    "    }\n",
    "    report = ProfileReport(df=ac, title=\"Flights Data\")\n",
    "    report.to_notebook_iframe()\n",
    "\n",
    "    df_cleaned, error_reports, _ = analyze_conversion_errors(ac, convert_dtypes)\n",
    "    print_error_report(error_reports)\n",
    "\n",
    "    # === Clean Known Numeric Columns ===\n",
    "    numeric_columns = ['AIR_TIME', 'DISTANCE', 'OCCUPANCY_RATE']\n",
    "    ac = apply_cleaning_to_numeric_columns(ac, numeric_columns)\n",
    "\n",
    "    # Fix dates manually\n",
    "    ac['FL_DATE'] = pd.to_datetime(ac['FL_DATE'], errors='coerce')\n",
    "    print(\"Error check after conversion\")\n",
    "    # Recheck Conversion Errors\n",
    "    df_cleaned, error_reports, _ = analyze_conversion_errors(ac, convert_dtypes)\n",
    "    print_error_report(error_reports)\n",
    "\n",
    "    # === Initial Null Check ===\n",
    "    plot_missing_values(ac)\n",
    "    null_values, na_cols = check_nulls(ac)\n",
    "\n",
    "    # === Filter Non-Cancelled Flights ===\n",
    "    filter_dict = {'CANCELLED': [0.0]}\n",
    "    ac, cat_cols_na, num_cols_na = filter_dataset_based_on_user(ac, filter_dict, na_cols)\n",
    "\n",
    "    # === Impute ===\n",
    "    ac = multi_strategy_imputer(ac, num_cols=num_cols_na, cat_cols=cat_cols_na,\n",
    "                                 num_strategy='median', cat_strategy='constant', constant_fill='Unknown')\n",
    "\n",
    "    # === Drop null-heavy columns ===\n",
    "    ac, _ = drop_all_null_columns(ac)\n",
    "    ac, _ = drop_high_null_columns(ac, 99)\n",
    "\n",
    "    # === Dedupe ===\n",
    "    ac, _ = check_and_handle_duplicates(ac)\n",
    "\n",
    "    # === Final Null Check + Impute ===\n",
    "    plot_missing_values(ac)\n",
    "    null_values, na_cols = check_nulls(ac)\n",
    "    na_col_types = classify_columns(ac[na_cols])\n",
    "    ac = multi_strategy_imputer(ac,\n",
    "                                num_cols=na_col_types['numerical'],\n",
    "                                cat_cols=na_col_types['categorical'],\n",
    "                                num_strategy='mean', cat_strategy='constant', constant_fill='Unknown')\n",
    "\n",
    "    # === Outlier Summary & Capping ===\n",
    "    all_col_types = classify_columns(ac)\n",
    "    outlier_cols = all_col_types['numerical']\n",
    "    print(\"\\n Outlier Summary Report:\")\n",
    "    report = outlier_summary_report(ac, outlier_cols, method='iqr')\n",
    "    print(report)\n",
    "    batch_plot_outliers(ac, outlier_cols, method='iqr')\n",
    "    #columns_to_cap = ['DEP_DELAY', 'ARR_DELAY', 'AIR_TIME', 'DISTANCE', 'OCCUPANCY_RATE']\n",
    "    for col in outlier_cols:\n",
    "        ac = cap_and_report_outliers(ac, col, k=3)\n",
    "        #print_outlier_summary(ac, col, k=3)\n",
    "        \n",
    "    batch_plot_outliers(ac, outlier_cols, method='iqr')\n",
    "\n",
    "    # === Save Cleaned ===\n",
    "    cleaned_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'cleaned', 'flights_cleaned.csv'))\n",
    "    ac.to_csv(cleaned_path, index=False)\n",
    "    print(f\"\\n Cleaned flights dataset saved to: {cleaned_path}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tickets QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dataset\n",
    "# === Load Tickets Data ===\n",
    "file_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'raw', 'Tickets.csv'))\n",
    "print(f\"\\n Loading data from: {file_path}\")\n",
    "tickets_df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Define dtypes and run conversion analysis\n",
    "convert_dtypes = {\n",
    "    'ITIN_ID': 'string', 'YEAR': 'string', 'QUARTER': 'string', 'ORIGIN': 'string', 'ORIGIN_COUNTRY': 'string',\n",
    "    'ORIGIN_STATE_ABR': 'string', 'ORIGIN_STATE_NM': 'string', 'ROUNDTRIP': 'float',\n",
    "    'REPORTING_CARRIER': 'string', 'PASSENGERS': 'int', 'ITIN_FARE': 'float', 'DESTINATION': 'string'\n",
    "}\n",
    "\n",
    "report = ProfileReport(df=tickets_df, title=\"Tickets Data\")\n",
    "report.to_notebook_iframe()\n",
    "\n",
    "print(\"\\n Running Conversion Error Analysis...\")\n",
    "df_cleaned, error_reports, _ = analyze_conversion_errors(tickets_df, convert_dtypes)\n",
    "print_error_report(error_reports)\n",
    "\n",
    "# Step 2: Apply numeric cleanup and standardization\n",
    "numeric_cols = ['ROUNDTRIP', 'PASSENGERS', 'ITIN_FARE']\n",
    "tickets_df = apply_cleaning_to_numeric_columns(tickets_df, numeric_cols)\n",
    "\n",
    "# Step 3: Recheck errors\n",
    "print(\"Error check after conversion\")\n",
    "df_cleaned, error_reports, _ = analyze_conversion_errors(tickets_df, convert_dtypes)\n",
    "print(\"\\n Final Conversion Check After Cleaning\")\n",
    "print_error_report(error_reports)\n",
    "\n",
    "# Step 4: Null checks\n",
    "plot_missing_values(tickets_df)\n",
    "null_summary, na_cols = check_nulls(tickets_df)\n",
    "print(\"\\n Null Summary:\")\n",
    "print(null_summary)\n",
    "\n",
    "# Step 5: Filter for ROUNDTRIP == 1.0\n",
    "filter_dict = {'ROUNDTRIP': [1.0]}\n",
    "tickets_df, cat_na_after_filter, num_na_after_filter = filter_dataset_based_on_user(tickets_df, filter_dict, na_cols)\n",
    "\n",
    "# Step 6: Imputation\n",
    "tickets_df = multi_strategy_imputer(\n",
    "    tickets_df,\n",
    "    num_cols=num_na_after_filter,\n",
    "    cat_cols=cat_na_after_filter,\n",
    "    num_strategy=\"median\",\n",
    "    cat_strategy=\"constant\",\n",
    "    constant_fill=\"Unknown\"\n",
    ")\n",
    "\n",
    "# Step 7: Drop null-heavy columns\n",
    "tickets_df, _ = drop_all_null_columns(tickets_df)\n",
    "tickets_df, _ = drop_high_null_columns(tickets_df, threshold=99)\n",
    "\n",
    "# Step 8: Remove duplicates\n",
    "tickets_df, _ = check_and_handle_duplicates(tickets_df)\n",
    "\n",
    "# Step 9: Final impute if anything remains\n",
    "plot_missing_values(tickets_df)\n",
    "null_summary, na_cols = check_nulls(tickets_df)\n",
    "cat_na_final = classify_columns(tickets_df[na_cols])\n",
    "tickets_df = multi_strategy_imputer(\n",
    "    tickets_df,\n",
    "    num_cols=cat_na_final['numerical'],\n",
    "    cat_cols=cat_na_final['categorical'],\n",
    "    num_strategy=\"median\",\n",
    "    cat_strategy=\"constant\",\n",
    "    constant_fill=\"Unknown\"\n",
    ")\n",
    "\n",
    "# Step 10: Outlier Treatment\n",
    "classified_cols = classify_columns(tickets_df)\n",
    "outlier_cols = [col for col in classified_cols['numerical'] if tickets_df[col].nunique() > 1]\n",
    "\n",
    "print(\"\\ Outlier Summary Before Capping:\")\n",
    "outlier_df = outlier_summary_report(tickets_df, outlier_cols, method='iqr')\n",
    "print(outlier_df)\n",
    "batch_plot_outliers(tickets_df, outlier_cols, method='iqr')\n",
    "\n",
    "for col in outlier_cols:\n",
    "    tickets_df = cap_and_report_outliers(tickets_df, col, k=3)\n",
    "batch_plot_outliers(tickets_df, outlier_cols, method='iqr')\n",
    "    #print_outlier_summary(tickets_df, col, k=3)\n",
    "\n",
    "# Step 11: Save Cleaned File\n",
    "cleaned_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'cleaned', 'tickets_cleaned.csv'))\n",
    "tickets_df.to_csv(cleaned_path, index=False)\n",
    "print(f\"\\n Cleaned Tickets Data saved to: {cleaned_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
